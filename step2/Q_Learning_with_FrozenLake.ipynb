{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5aQKQMJTJBPH"
   },
   "source": [
    "# Q* Learning with FrozenLake 4x4 \n",
    "\n",
    "In this Notebook, we'll implement an agent <b>that plays FrozenLake.</b>\n",
    "\n",
    "![alt text](http://simoninithomas.com/drlc/Qlearning/frozenlake4x4.png)\n",
    "\n",
    "The goal of this game is <b>to go from the starting state (S) to the goal state (G)</b> by walking only on frozen tiles (F) and avoid holes (H). However, the ice is slippery, **so you won't always move in the direction you intend (stochastic environment)**\n",
    "\n",
    "Thanks to [lukewys](https://github.com/lukewys) for his help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QK8fD0zAQkkg"
   },
   "source": [
    "## Prerequisites üèóÔ∏è\n",
    "Before diving on the notebook **you need to understand**:\n",
    "- The foundations of Reinforcement learning (MC, TD, Rewards hypothesis...) [Article](https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419)\n",
    "- Q-learning [Article](https://medium.freecodecamp.org/diving-deeper-into-reinforcement-learning-with-q-learning-c18d0db58efe)\n",
    "- In the [video version](https://www.youtube.com/watch?v=q2ZOEFAaaI0)  we implemented a Q-learning agent that learns to play OpenAI Taxi-v2 üöï with Numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xr9nI6dcQM8I"
   },
   "source": [
    "# This is a notebook from [Deep Reinforcement Learning Course, new version](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/)\n",
    "<img src=\"https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/docs/assets/img/DRLC%20Environments.png\" alt=\"Deep Reinforcement Course\"/>\n",
    "<br>\n",
    "<p>  Deep Reinforcement Learning Course is a free series of articles and videos tutorials üÜï about Deep Reinforcement Learning, where **we'll learn the main algorithms (Q-learning, Deep Q Nets, Dueling Deep Q Nets, Policy Gradients, A2C, Proximal Policy Gradients, Prediction Based rewards agents‚Ä¶), and how to implement them with Tensorflow and PyTorch.**\n",
    "\n",
    "  ![alt text](http://simoninithomas.com/drlc/libraries.png)\n",
    "  \n",
    "  \n",
    "<br><br>\n",
    "    \n",
    "üìúThe articles explain the architectures from the big picture to the mathematical details behind them.\n",
    "<br>\n",
    "üìπ The videos explain how to build the agents with Tensorflow </b></p>\n",
    "<br>\n",
    "This course will give you a **solid foundation for understanding and implementing the future state of the art algorithms**. And, you'll build a strong professional portfolio by creating **agents that learn to play awesome environments**: Doom¬© üëπ, Space invaders üëæ, Outrun, Sonic the Hedgehog¬©, Michael Jackson‚Äôs Moonwalker, agents that will be able to navigate in 3D environments with DeepMindLab (Quake) and able to walk with Mujoco. \n",
    "<br><br>\n",
    "</p> \n",
    "\n",
    "## üìö The complete [Syllabus HERE](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/)\n",
    "\n",
    "\n",
    "## Any questions üë®‚Äçüíª\n",
    "<p> If you have any questions, feel free to ask me: </p>\n",
    "<p> üìß: <a href=\"mailto:hello@simoninithomas.com\">hello@simoninithomas.com</a>  </p>\n",
    "<p> Github: https://github.com/simoninithomas/Deep_reinforcement_learning_Course </p>\n",
    "<p> üåê : https://simoninithomas.github.io/Deep_reinforcement_learning_Course/ </p>\n",
    "<p> Twitter: <a href=\"https://twitter.com/ThomasSimonini\">@ThomasSimonini</a> </p>\n",
    "<p> Don't forget to <b> follow me on <a href=\"https://twitter.com/ThomasSimonini\">twitter</a>, <a href=\"https://github.com/simoninithomas/Deep_reinforcement_learning_Course\">github</a> and <a href=\"https://medium.com/@thomassimonini\">Medium</a> to be alerted of the new articles that I publish </b></p>\n",
    "    \n",
    "## How to help  üôå\n",
    "3 ways:\n",
    "- **Clap our articles and like our videos a lot**:Clapping in Medium means that you really like our articles. And the more claps we have, the more our article is shared Liking our videos help them to be much more visible to the deep learning community.\n",
    "- **Share and speak about our articles and videos**: By sharing our articles and videos you help us to spread the word. \n",
    "- **Improve our notebooks**: if you found a bug or **a better implementation** you can send a pull request.\n",
    "<br>\n",
    "\n",
    "## Important note ü§î\n",
    "<b> You can run it on your computer but it's better to run it on GPU based services</b>, personally I use Microsoft Azure and their Deep Learning Virtual Machine (they offer 170$)\n",
    "https://azuremarketplace.microsoft.com/en-us/marketplace/apps/microsoft-ads.dsvm-deep-learning\n",
    "<br>\n",
    "‚ö†Ô∏è I don't have any business relations with them. I just loved their excellent customer service.\n",
    "\n",
    "If you have some troubles to use Microsoft Azure follow the explainations of this excellent article here (without last the part fast.ai): https://medium.com/@manikantayadunanda/setting-up-deeplearning-machine-and-fast-ai-on-azure-a22eb6bd6429"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "54AIvDov_7aa"
   },
   "source": [
    "## Step -1: Install the dependencies on Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "gxxpHDIs_lvg",
    "outputId": "092735c5-16e4-4f5f-f7af-7d189c75dbb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/ms/.pyenv/versions/3.6.8/envs/reinforced-course/lib/python3.6/site-packages (1.16.1)\n",
      "Collecting gym\n",
      "  Using cached https://files.pythonhosted.org/packages/87/04/70d4901b7105082c9742acd64728342f6da7cd471572fd0660a73f9cfe27/gym-0.10.11.tar.gz\n",
      "Collecting scipy (from gym)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/1d/eef9d7b34ab8b7ee42d570f2e24d58ee0374064c1ca593bdb02914f66a80/scipy-1.2.0-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (28.8MB)\n",
      "\u001b[K    100% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28.8MB 792kB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.10.4 in /Users/ms/.pyenv/versions/3.6.8/envs/reinforced-course/lib/python3.6/site-packages (from gym) (1.16.1)\n",
      "Collecting requests>=2.0 (from gym)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/e3/20f3d364d6c8e5d2353c72a67778eb189176f08e873c9900e10c0287b84b/requests-2.21.0-py2.py3-none-any.whl (57kB)\n",
      "\u001b[K    100% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 61kB 6.9MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: six in /Users/ms/.pyenv/versions/3.6.8/envs/reinforced-course/lib/python3.6/site-packages (from gym) (1.12.0)\n",
      "Collecting pyglet>=1.2.0 (from gym)\n",
      "  Using cached https://files.pythonhosted.org/packages/1c/fc/dad5eaaab68f0c21e2f906a94ddb98175662cc5a654eee404d59554ce0fa/pyglet-1.3.2-py2.py3-none-any.whl\n",
      "Collecting urllib3<1.25,>=1.21.1 (from requests>=2.0->gym)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/00/ee1d7de624db8ba7090d1226aebefab96a2c71cd5cfa7629d6ad3f61b79e/urllib3-1.24.1-py2.py3-none-any.whl (118kB)\n",
      "\u001b[K    100% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 122kB 10.3MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting idna<2.9,>=2.5 (from requests>=2.0->gym)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\n",
      "\u001b[K    100% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 61kB 6.6MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting chardet<3.1.0,>=3.0.2 (from requests>=2.0->gym)\n",
      "  Using cached https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.0->gym)\n",
      "  Using cached https://files.pythonhosted.org/packages/9f/e0/accfc1b56b57e9750eba272e24c4dddeac86852c2bebd1236674d7887e8a/certifi-2018.11.29-py2.py3-none-any.whl\n",
      "Collecting future (from pyglet>=1.2.0->gym)\n",
      "  Using cached https://files.pythonhosted.org/packages/90/52/e20466b85000a181e1e144fd8305caf2cf475e2f9674e797b222f8105f5f/future-0.17.1.tar.gz\n",
      "Building wheels for collected packages: gym, future\n",
      "  Building wheel for gym (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/ms/Library/Caches/pip/wheels/7b/eb/1f/22c4124f3c64943aa0646daf4612b1c1f00f27d89b81304ebd\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/ms/Library/Caches/pip/wheels/0c/61/d2/d6b7317325828fbb39ee6ad559dbe4664d0896da4721bf379e\n",
      "Successfully built gym future\n",
      "Installing collected packages: scipy, urllib3, idna, chardet, certifi, requests, future, pyglet, gym\n",
      "Successfully installed certifi-2018.11.29 chardet-3.0.4 future-0.17.1 gym-0.10.11 idna-2.8 pyglet-1.3.2 requests-2.21.0 scipy-1.2.0 urllib3-1.24.1\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G9qH33L_QoBk"
   },
   "source": [
    "## Step 0: Import the dependencies üìö\n",
    "We use 3 libraries:\n",
    "- `Numpy` for our Qtable\n",
    "- `OpenAI Gym` for our FrozenLake Environment\n",
    "- `Random` to generate random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oU8zRXv8QHlm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0fz-X3HTQueX"
   },
   "source": [
    "## Step 1: Create the environment üéÆ\n",
    "- Here we'll create the FrozenLake 8x8 environment. \n",
    "- OpenAI Gym is a library <b> composed of many environments that we can use to train our agents.</b>\n",
    "- In our case we choose to use Frozen Lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0pVMyaVIQF5C"
   },
   "outputs": [],
   "source": [
    "from gym.envs.registration import register\n",
    "\n",
    "register(\n",
    "    id='a1-v3',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name' : '8x8', 'is_slippery': False},\n",
    "    max_episode_steps=100,\n",
    "    reward_threshold=0.8196, # optimum = .8196, changing this seems have no influence\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mh9jBR_cQ5_a"
   },
   "outputs": [],
   "source": [
    "# env = gym.make(\"FrozenLakeNotSlippery-v0\")\n",
    "\n",
    "#env = gym.make(\"FrozenLake-v0\")\n",
    "\n",
    "env = gym.make(\"a1-v3\")\n",
    "\n",
    "# env = gym.make(\"Taxi-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "xdSrj12X_y2D",
    "outputId": "1ccbddc9-a1d8-4948-d216-9459598aae1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "States =  64 , Actions =  4\n"
     ]
    }
   ],
   "source": [
    "env.render()\n",
    "print(\"States = \", env.observation_space.n, \", Actions = \", env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JEtXMldxQ7uw"
   },
   "source": [
    "## Step 2: Create the Q-table and initialize it üóÑÔ∏è\n",
    "- Now, we'll create our Q-table, to know how much rows (states) and columns (actions) we need, we need to calculate the action_size and the state_size\n",
    "- OpenAI Gym provides us a way to do that: `env.action_space.n` and `env.observation_space.n`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uc0xDVd_Q-C8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pj-5iUhnBJRs"
   },
   "source": [
    "## 3. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z-UfTY81__aC"
   },
   "outputs": [],
   "source": [
    "max_steps = 100\n",
    "epochs = 100000\n",
    "\n",
    "learning_rate    = 0.8\n",
    "discounting      = 0.95  # gamma\n",
    "\n",
    "max_exploration   = 1\n",
    "min_exploration   = 0.01\n",
    "exploration_decay = 0.001\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6bTBsZFbB3WF"
   },
   "source": [
    "## 4. Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sGcIDJalBNs8"
   },
   "outputs": [],
   "source": [
    "def chose_action(state, exploration_rate):\n",
    "  if random.uniform(0, 1) < exploration_rate:\n",
    "    return env.action_space.sample()\n",
    "  else:\n",
    "    action = np.argmax(qtable[state, :])\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kvu8-DsIFKH1"
   },
   "outputs": [],
   "source": [
    "def learn(state, action, next_state, resulting_reward):\n",
    "  predicted = qtable[state, action]\n",
    "  delta_q = resulting_reward + discounting * np.max(qtable[next_state]) - predicted\n",
    "  qtable[state, action] = predicted + learning_rate*delta_q\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RoT3UQ2bJWGw"
   },
   "outputs": [],
   "source": [
    "def learn_epoch(max_steps, exploration_rate):\n",
    "  state = env.reset()\n",
    "  total_reward = 0\n",
    "  for s in range(max_steps):\n",
    "    action = chose_action(state, exploration_rate)\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    if reward == 0 and done:\n",
    "      reward = -1\n",
    "    elif new_state == state:\n",
    "      reward -= 0.01\n",
    "    total_reward += reward\n",
    "    learn(state, action, new_state, reward)\n",
    "    state = new_state\n",
    "    if done:\n",
    "      break;\n",
    "  return total_reward, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TH_WgxmrB2X_"
   },
   "outputs": [],
   "source": [
    "def learn_qtable(epochs, max_steps, print_debug=False):\n",
    "  segment_size = epochs / 20\n",
    "  segment_reward = 0\n",
    "  for e in range(epochs):\n",
    "    exploration_rate = min_exploration + (max_exploration-min_exploration)*np.exp(-exploration_decay*e)\n",
    "    total_reward, final_state = learn_epoch(max_steps, exploration_rate)\n",
    "    segment_reward += total_reward\n",
    "    if print_debug and e % segment_size == 0:\n",
    "      clear_output()\n",
    "      print(qtable)\n",
    "    if e % segment_size == 0:\n",
    "      print(\"Epoch=\", e, \", Exploration Rate=\", exploration_rate, \" avg reward=\", segment_reward/segment_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1462
    },
    "colab_type": "code",
    "id": "nBlD40gbHLyf",
    "outputId": "e1a3311a-c588-48d5-e430-8248be1ac480"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch= 0 , Exploration Rate= 1.0  avg reward= -0.000218\n",
      "Epoch= 5000 , Exploration Rate= 0.016670567529094613  avg reward= 0.029374000000003828\n",
      "Epoch= 10000 , Exploration Rate= 0.010044945930464861  avg reward= 0.9752180000000022\n",
      "Epoch= 15000 , Exploration Rate= 0.010000302843297297  avg reward= 1.927877999999999\n",
      "Epoch= 20000 , Exploration Rate= 0.010000002040542086  avg reward= 2.8761679999999967\n",
      "Epoch= 25000 , Exploration Rate= 0.010000000013749065  avg reward= 3.830838000000007\n",
      "Epoch= 30000 , Exploration Rate= 0.010000000000092641  avg reward= 4.778298000000028\n",
      "Epoch= 35000 , Exploration Rate= 0.010000000000000625  avg reward= 5.73130000000006\n",
      "Epoch= 40000 , Exploration Rate= 0.010000000000000004  avg reward= 6.69031600000008\n",
      "Epoch= 45000 , Exploration Rate= 0.01  avg reward= 7.644940000000046\n",
      "Epoch= 50000 , Exploration Rate= 0.01  avg reward= 8.595574000000013\n",
      "Epoch= 55000 , Exploration Rate= 0.01  avg reward= 9.547433999999985\n",
      "Epoch= 60000 , Exploration Rate= 0.01  avg reward= 10.504469999999952\n",
      "Epoch= 65000 , Exploration Rate= 0.01  avg reward= 11.457895999999918\n",
      "Epoch= 70000 , Exploration Rate= 0.01  avg reward= 12.416955999999892\n",
      "Epoch= 75000 , Exploration Rate= 0.01  avg reward= 13.373621999999889\n",
      "Epoch= 80000 , Exploration Rate= 0.01  avg reward= 14.324273999999967\n",
      "Epoch= 85000 , Exploration Rate= 0.01  avg reward= 15.282114000000051\n",
      "Epoch= 90000 , Exploration Rate= 0.01  avg reward= 16.238770000000127\n",
      "Epoch= 95000 , Exploration Rate= 0.01  avg reward= 17.194588000000223\n",
      "final:\n",
      "[[ 0.478  0.513  0.513  0.478]\n",
      " [ 0.39   0.54   0.44   0.499]\n",
      " [ 0.334  0.411  0.463  0.413]\n",
      " [ 0.346  0.488  0.     0.361]\n",
      " [ 0.     0.     0.    -0.01 ]\n",
      " [ 0.     0.     0.    -0.01 ]\n",
      " [ 0.     0.     0.    -0.01 ]\n",
      " [ 0.     0.    -0.01  -0.01 ]\n",
      " [ 0.503  0.54   0.54   0.488]\n",
      " [ 0.513  0.569  0.513  0.494]\n",
      " [ 0.54   0.46   0.468 -0.   ]\n",
      " [ 0.513 -1.     0.     0.445]\n",
      " [ 0.39   0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.   ]\n",
      " [ 0.     0.    -0.01   0.   ]\n",
      " [ 0.53   0.569  0.569  0.513]\n",
      " [ 0.54   0.599  0.599  0.54 ]\n",
      " [ 0.569  0.63  -1.     0.513]\n",
      " [ 0.     0.     0.     0.   ]\n",
      " [-1.     0.698  0.403  0.   ]\n",
      " [ 0.637 -0.998  0.     0.   ]\n",
      " [ 0.     0.     0.    -0.16 ]\n",
      " [ 0.     0.    -0.01   0.   ]\n",
      " [ 0.559  0.54   0.599  0.54 ]\n",
      " [ 0.569  0.569  0.63   0.569]\n",
      " [ 0.599  0.599  0.663  0.599]\n",
      " [ 0.63  -1.     0.698 -1.   ]\n",
      " [ 0.663  0.735 -1.     0.663]\n",
      " [ 0.     0.     0.     0.   ]\n",
      " [-0.96   0.     0.     0.   ]\n",
      " [ 0.     0.    -0.01   0.   ]\n",
      " [-0.01  -0.032  0.569  0.455]\n",
      " [-0.    -1.     0.599  0.599]\n",
      " [ 0.379 -1.    -1.     0.63 ]\n",
      " [ 0.     0.     0.     0.   ]\n",
      " [-1.     0.774  0.774  0.698]\n",
      " [ 0.735  0.815  0.    -1.   ]\n",
      " [ 0.558  0.     0.     0.   ]\n",
      " [ 0.     0.    -0.008  0.   ]\n",
      " [-0.01  -0.    -1.     0.432]\n",
      " [ 0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.   ]\n",
      " [-0.992 -0.     0.774 -1.   ]\n",
      " [ 0.735 -1.     0.815  0.735]\n",
      " [ 0.774  0.857 -1.     0.774]\n",
      " [ 0.     0.     0.     0.   ]\n",
      " [-0.8    0.     0.     0.   ]\n",
      " [-0.01  -0.    -1.    -0.   ]\n",
      " [ 0.     0.     0.     0.   ]\n",
      " [-1.    -0.008 -0.032 -1.   ]\n",
      " [-0.    -1.    -1.    -0.8  ]\n",
      " [ 0.     0.     0.     0.   ]\n",
      " [-1.     0.902 -1.     0.815]\n",
      " [ 0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.   ]\n",
      " [-0.01  -0.01  -0.    -0.16 ]\n",
      " [-0.    -0.01  -0.    -1.   ]\n",
      " [-0.8   -0.074 -1.    -0.16 ]\n",
      " [ 0.     0.     0.     0.   ]\n",
      " [-0.96  -0.01   0.902 -0.992]\n",
      " [ 0.857  0.892  0.95   0.857]\n",
      " [ 0.902  0.94   1.    -1.   ]\n",
      " [ 0.     0.     0.     0.   ]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "qtable = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "learn_qtable(epochs, max_steps, print_debug=False)\n",
    "\n",
    "print(\"final:\")\n",
    "print(qtable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "byAWsfKnD3WA"
   },
   "outputs": [],
   "source": [
    "def execute(max_steps):\n",
    "  state = env.reset()\n",
    "  for s in range(max_steps):\n",
    "    action = chose_action(state, -100)\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    state = new_state\n",
    "    env.render()\n",
    "    if (done):\n",
    "      print(\"Done after \", s, \" steps\")\n",
    "      break\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2176
    },
    "colab_type": "code",
    "id": "4KtFx37tH7cR",
    "outputId": "c620ae5a-a69d-4c44-d20d-3daa9772ad01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "F\u001b[41mF\u001b[0mFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FF\u001b[41mF\u001b[0mFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFF\u001b[41mF\u001b[0mFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFF\u001b[41mF\u001b[0mHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFH\u001b[41mF\u001b[0mFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHF\u001b[41mF\u001b[0mFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFF\u001b[41mF\u001b[0mHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFH\u001b[41mF\u001b[0mHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHF\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFF\u001b[41mG\u001b[0m\n",
      "Done after  13  steps\n"
     ]
    }
   ],
   "source": [
    "execute(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BOZN0OASH8U6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Q* Learning with FrozenLake.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
